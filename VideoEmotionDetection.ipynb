{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "\n",
        "class EmotionDetector:\n",
        "    def __init__(self, model_path, cascade_path):\n",
        "        # Load the emotion detection model\n",
        "        self.model = load_model(model_path)\n",
        "        # Mapping of emotion labels to their corresponding indices\n",
        "        self.emotions_mapping = {0: 'anger',\n",
        "                                 1: 'contempt',\n",
        "                                 2: 'disgust',\n",
        "                                 3: 'fear',\n",
        "                                 4: 'happy',\n",
        "                                 5: 'neutral',\n",
        "                                 6: 'sad',\n",
        "                                 7: 'surprise',\n",
        "                                 8: 'uncertain'}\n",
        "        # Load the face cascade classifier\n",
        "        self.face_cascade = cv2.CascadeClassifier(cascade_path)\n",
        "\n",
        "    def preprocess_frame(self, face_roi):\n",
        "        # Resize the face region of interest (ROI) to the required input size\n",
        "        face_resized = cv2.resize(face_roi, (224, 224))\n",
        "        # Convert the resized face ROI to RGB and normalize pixel values\n",
        "        face_resized_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)\n",
        "        img_array = img_to_array(face_resized_rgb)\n",
        "        img_array = preprocess_input(img_array)\n",
        "        face = np.expand_dims(img_array, axis=0)\n",
        "        return face\n",
        "\n",
        "    def predict_emotion(self, face_roi):\n",
        "        # Preprocess the face ROI\n",
        "        preprocessed_frame = self.preprocess_frame(face_roi)\n",
        "        # Make predictions using the emotion detection model\n",
        "        predictions = self.model(preprocessed_frame)\n",
        "        # Get the predicted emotion label\n",
        "        predicted_emotion = self.emotions_mapping[np.argmax(predictions)]\n",
        "        return predicted_emotion\n",
        "\n",
        "    def process_video(self, input_path, output_path):\n",
        "        # Open the video file for reading\n",
        "        video_capture = cv2.VideoCapture(input_path)\n",
        "        # Get the frames per second of the video\n",
        "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
        "        # Open the video file for reading\n",
        "        frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        # Create a VideoWriter object to save the processed video\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "        # Process each frame in the video\n",
        "        while True:\n",
        "            # Read a frame from the video\n",
        "            ret, frame = video_capture.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert the frame to grayscale\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            # Detect faces in the grayscale frame\n",
        "            faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "            # Process each detected face\n",
        "            for (x, y, w, h) in faces:\n",
        "                # Extract the face region of interest (ROI)\n",
        "                face_roi = frame[y:y+h, x:x+w]\n",
        "                # Extract the face region of interest (ROI)\n",
        "                emotion = self.predict_emotion(face_roi)\n",
        "                # Draw a bounding box around the face and display the predicted emotion\n",
        "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "            out.write(frame)\n",
        "\n",
        "        video_capture.release()\n",
        "        out.release()\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "5HjMmb4Clgf2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths for the emotion detection model and face cascade classifier\n",
        "model_path = 'best_model.h5' # Path to CV-model\n",
        "cascade_path = 'haarcascade_frontalface_default.xml' # Path to \"Haar Cascades\"-xml file\n",
        "\n",
        "# Initialize the emotion detector with the model and cascade paths\n",
        "emotion_detector = EmotionDetector(model_path, cascade_path)"
      ],
      "metadata": {
        "id": "IFUJzJ0CmiID"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the video and save the processed video with bounding boxes and emotions\n",
        "input_video_path = 'input_video.mp4'\n",
        "output_video_path = 'output_video.mp4'\n",
        "emotion_detector.process_video(input_video_path, output_video_path)"
      ],
      "metadata": {
        "id": "bLSHNCoBPWJa"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}